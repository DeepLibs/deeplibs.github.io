<!DOCTYPE html>
<html>

<head>
    <title>Backpropagation</title>
    <meta charset="UTF-8">
    <meta name="description" content="Backpropagation">
    <meta name="author" content="Thomio Watanabe">
    <link rel="stylesheet" type="text/css" href="../style/deeplibs.css">
</head>

<body>

<header>
    <a href="../index.html" style="color: white; font-size:20px;">
        <b style="display: inline-block; width: 768px">DeepLibs</b>
    </a>
</header>


<article>
<h1>Backpropagation</h1>

<p>
Alongside Gradient Descent, backpropagation is one of the most important
 algorithms for artificial neural networks.
A single artificial neuron is not able to model real world problems and to
 achive real world representations we must stack neurons in layers.
The more layers a network has the deeper it is and as a general rule of thumb
deeper networks are more accurate than shallow ones.
Backpropagation is the algorithm that makes possible to adjust the weights from
 neurons in the early layers (front layers).
</p>

<p>
The Backpropagation is an old algorithm dated from 1970s but only in 1986 with
 the publishing of the paper "Learning Representations by Back-Propagating Errors"
 by Rumelhart, Hinton, and Williams that it was applied with neural networks.
Although, it is an old algorithm backpropagation has being used in all deep
 learning models.
</p>

<p>
Geoffrey Hinton, on of the backpropagation creators, is an important figure in
 the machine learning community.
He has been working with artificial neural networks for a long time and he was
 also one of the responsibles for the AlexNet network.
Curiously, he is actively criticizing deep learning basic building blocks,
 precisely, the max pooling layer and the backpropagation algorithm.
According to Hinton, the current models are not able to learn spatial
 hierarchies between simple and complex objects.
One typical example is that we must provide images of the same object in many
 different angles, size and locations.
</p>

<p>
To make models learn similarly to what the human brain does in practice, Hinton
 and his team have introduced the
 <a href="https://arxiv.org/abs/1710.09829">capsules network</a>.
A capsule is a building block introduced to replace Convolutional kernels.
And in order to train this network they also had to replace the backpropagation
 algorithm for another algorithm called <q>dynamic routing between capsules</q>.
Initial experiments demonstrate that capsules network are more data efficient
but they are is still slower than the usual Convolutional Neural Networks (CNN).
Besides its computational requirements the capsules network need to be tested
 on different datasets and applications.
Then, we will have a better idea if it can replace the CNNs.
For a complete overview about the capsules network check this
<a href="https://medium.com/aiÂ³-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b">
article.
</a>

</p>

</article>


<footer>
    <br>
    <div style="display: inline-block; width: 768px">
        Copyright &copy; Thomio Watanabe
    </div>
</footer>

</body>

</html>
