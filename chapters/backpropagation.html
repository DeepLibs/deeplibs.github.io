<!DOCTYPE html>
<html>

<head>
    <title>Backpropagation</title>
    <meta charset="UTF-8">
    <meta name="description" content="Backpropagation">
    <meta name="author" content="Thomio Watanabe">
    <link rel="stylesheet" type="text/css" href="../style/deeplibs.css">
</head>

<body>

<header>
    <a href="../index.html" style="color: white; font-size:20px;">
        <b style="display: inline-block; width: 768px">DeepLibs</b>
    </a>
</header>


<article>
<h1>Backpropagation</h1>

<br>

<p>
Alongside Gradient Descent, backpropagation is one of the most important
 algorithms for artificial neural networks.
A single artificial neuron is not able to model real world problems and to
 achive real world representations we must stack neurons in layers.
The more layers a network has the deeper it is and as a general rule of thumb
deeper networks are more accurate.
Backpropagation is the algorithm that makes possible to adjust the weights from
 neurons in the early layers (front layers).
</p>

<p>
The Backpropagation is an old algorithm dated from 1970s but only in 1986 with
 the publishing of the paper "Learning Representations by Back-Propagating Errors"
 by Rumelhart, Hinton, and Williams that it was applied with neural networks.
</p>
</article>



<div id="copyright">
    Copyright &copy; Thomio Watanabe
</div>

</body>

</html>
