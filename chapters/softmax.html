<!DOCTYPE html>
<html>

<!--Last update: Feb 7, 2018-->
<head>
    <title>Softmax</title>
    <meta charset="UTF-8">
    <meta name="description" content="Softmax">
    <meta name="author" content="Thomio Watanabe">
    <link rel="stylesheet" type="text/css" href="../style/deeplibs.css">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>
<header>
    <a href="../index.html" style="color: white; font-size:20px;"><b>DeepLibs</b></a>
</header>

<article>
<h1>Softmax</h1>
<br>

<p>
Softmax is one of the most important layers in deep learning.
It is usually the last layer of the network and it is responsible to compute the
discrete probability distribution over network scores.
For the classification problem the softmax layer outputs confidence levels for
each output dimension (for each class).
</p>

<p>
For example, imagine a classification problem where we need to classify images
 in 5 classes.
The neural network must have 5 independent outputs each one representing a
 specific class.
The softmax function outputs 5 confidence levels and the sum of these confidence
is equal to 1.
Since the output is a probability distribution the output with highest confidence
is the most likely class of the image.
</p>


<p>
The softmax function \(s_{i}\) is defined as:
$$ s_{i} = \frac{ e^{x_{i}} } { \sum_{i=1}^{N} { e^{x_{i}} } }$$

The index \(i\) represents a class, \(x_{i}\) is the network output for the
 class \(i\) and \(N\) is the total number of classes.

It is important to note that softmax output of each class considers the output
 of the other classes.
In other words, the confidence levels consider the score from all classes.
</p>


<p>
By the function definition we have:

$$ \sum_{i=1}^{N} s_{i} = 1 $$
</p>


<p>
Since the softmax function provide values between 0 and 1, and the sum of the
 confidences is equal to 1 we consider the softmax a true discrete probability
 distribution.
This property is not guaranteed with euclidean layers that use the mean squared
 error (MSE).
</p>


<p>
For a given image, if a network outputs the following scores
\( [0, 10, 15, 18, 20] \), the softmax output will be:

<table style="width:10cm"; align="center">
  <tr>
    <th>Class</th>
    <th>Score</th> 
    <th>Confidence</th>
  </tr>
  <tr>
    <td>0</td>
    <td>0</td>
    <td>0.0000</td>
  </tr>
  <tr>
    <td>1</td>
    <td>10</td>
    <td>0.0000</td>
  </tr>
  <tr>
    <td>2</td>
    <td>15</td>
    <td>0.0059</td>
  </tr>
  <tr>
    <td>3</td>
    <td>18</td>
    <td>0.1185</td>
  </tr>
  <tr>
    <td>4</td>
    <td>20</td>
    <td>0.8756</td>
  </tr>
</table>
</p>

<p>
With score of 20 and confidence level of 87.56% the most likely class is class
 number 4.
Note the softmax output is non-linear and classes with high scores tend to
 reduce the confidence of low scores classes.
Therefore the softmax function enforces the class separation in the network.
</p>


<p>
Since the softmax function considers the output of each class, even if the
correct class has a high score other classes with close score will decrease the
confidence level from the correct class.
For instance, if we increase the fourth class score from 18 to 19, the
 confidence level from the last class goes from 87.56% to 72.75%.
This minor modification could decrease the correct class confidence in more than
 10%.
</p>


<p>
Also, keep in mind that the usual output from a classification network is not a
probability distribution but the class of a given image.
Therefore is very common to use an argmax function after the sofmax.
The argmax function returns the argument(index) of the element with the highest
 confidence level.
</p>



<br>
<h3>Softmax loss</h3>

<p>
Since the softmax is usually the last layer from neural networks, it is also
 responsible
 to compute the error signal that will be used to optimize the network weights.
The softmax layer frequently uses the cross-entropy function (from information
theory) to compute the error signal.
</p>


<p>
Given a "correct" probatibilty distribution \(p\) and a "wrong" probability
 distribution \(q\) the cross-entropy function \(H\) is defined according to:
$$ H(p,q) = - \sum_{i=1}^{N}(p_{i} * log(q_{i})) $$
</p>


<p>
The cross-entropy function provides the error signal using the sum of the
 multiplication of two probabilities distributions.
The "correct" distribution are the labels (or ground truth) while the "wrong"
 distribution is the output of the softmax function.
</p>


<p>
The cross-entropy function allows the use of one hot encoding to define the
 labels.
One hot encoding has the great advantage to prevent the direct use of
 numerical values to encode classes.
In the technique all labels are equal to 0 but the element that represents the
 correct class, which is equal to 1.
From the last example, a valid label vector that considers the last index the
 correct class of one specific entry, would be equal to:
$$ labels  = p_{i} = [0, 0, 0, 0, 1] $$
Also from the last example we have the following softmax output:
$$ output = q_{i} = [ 0, 0,  0.0059, 0.1185, 0.8756] $$
From the cross-entropy function:
$$
H(p,q) = - \sum_{i=1}^{N}(p_{i} * log(q_{i}))\\
= - ( 0 * log(0) +
      0 * log(0) +
      0 * log(0.0059) +
      0 * log(0.1185) +
      1 * log(0.8756) )\\
= - ( 0 * (-20.1329) +
      0 * (-10.1329) +
      0 * (-5.1329) +
      0 * (-2.1329) +
      1 * (-0.1329) )\\
= - ( - 0.13288) = 0.13288
$$

Note the \(0\) argument in the \(log\) function it is a very small value close to
 \(0\).
</p>


    


<p>
Due to the one hot encoding only the confidence value from the correct class is
 computed.
However, as previously explained, the confidence levels consider the scores from
 all classes and therefore there is no need to utilize all confidence values in
 the error signal.
</p>


<p>
In this last example we had a label that was matching the highest confidence.
Lets make a simple experiment considering the third class as the correct class.
The label vector becomes:
$$ labels  = p_{i} = [0, 0, 1, 0, 0] $$

And the cross-entropy:
$$ H(p,q) = -( 1 * (-5.1329)) = 5.13288 $$

Basically, the error signal increases if the confidence level from the correct
 class is close to 0 and decreases if the confidence approaches 1.
</p>



<br>
<p>
A python code with examples can be found at
 <a href="https://github.com/DeepLibs/softmax">repository</a>.
</p>




</article>

<footer><br>Copyright &copy; Thomio Watanabe</footer>

</body>

</html>
