<!DOCTYPE html>
<html>

<head>
    <title>Perturbations</title>
    <meta charset="UTF-8">
    <meta name="description" content="Perturbations">
    <meta name="author" content="Thomio Watanabe">
    <link rel="stylesheet" type="text/css" href="../style/deeplibs.css">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>
<header>
    <a href="../index.html" style="color: white; font-size:20px;">
        <b style="display: inline-block; width: 768px">DeepLibs</b>
    </a>
</header>

<article>
    <h1>Perturbations</h1>
    <br>


<p>
While we still really (really) far from creating skynet and the terminator we
should keep in mind real problems that rises with the current state of the art
in Artificial Intelligence.
Besides being used for malicious purposes deep learning models can also be
 easily hacked.
</p>

<p>
<b>Perturbations</b> are one of the biggest threats to AI.
Basically a perturbation is any modification made in the input data with
the purpose to generate wrong results.
Perturbations can be subdivided in adversarial or universal.
</p>

<p>
Adversarial examples were introduced by Szegedy, et al., 2013 in
 <q>Intriguing properties of neural networks</q>
[<a href="https://arxiv.org/abs/1312.6199">arxiv</a>] when these researchers
 were investigating neural networks properties.
Adversarial examples are input data modified with small perturbations especially
designed to trick an AI model to output incorrect answers with high confidence.
For example, Figure 1 demonstrates a perturbation signal that was added
to misclassify a panda as a gibbon.
</p>

<figure>
    <img src="../figures/adversarial_example.png" align="middle"
    alt="Adversarial example" width="520" height="210" style="padding-left: 1.5cm">
    <figcaption align="middle"> <b>Figure 1</b> - Adversarial example trained
    with GoogLeNet and ImageNet. Image obtained from <q>Explaining and
    harnessing adversarial examples</q>, Goodfellow et al. 2015,
    [<a href="https://arxiv.org/abs/1412.6572">arxiv</a>].
    </figcaption>
</figure>


<p>
These modifications are in most cases imperceptible to the human eye.
For instance, the panda image on right is indistinguishable to the image on left.
This example demonstrates that machine learning models are able to learn and
identify patterns we are not able to see.
This situation poses an interesting question: does an AI model really needs to
identify patters we don't perceive ?
</p>


<p>
Perturbation signals can easily fool state of the art models.
The problem is very severe since anyone with a basic knowledge in deep learning
is able to create adversarial examples.
There are also several researches to create defenses on AI models but so far
these defenses do not adapt to the type of the attack and can be easily defeated.
</p>


<p>
<!--These problems rise because we rely on datasets to train an AI model.-->
There are several reasons for the existence of this problem.
Szegedy, et al. points out that these perturbations have a extremely low
probability to exist on the dataset.
It is impractical to create a dataset that contemplates every
single possible entry a model can receive.
If we had a better understanding of deep neural networks we could create
efficient countermeasures to perturbations.
</p>


<p>
A fun fact is that it is not only possible to fool machines but many projects 
are being explored to fool humans, see
 <q>Adversarial Examples that Fool both Human and Computer Vision</q>,
 Elsayed et al. 2018, [<a href="https://arxiv.org/abs/1802.08195">arxiv</a>].
For instance, there are researches to reproduce someone else's voice and
researches to replace faces.
These researches among others in AI are very polemic.
These tasks are not new but the results obtained with the use of AI are
impressive.
</p>


</article>

<footer>
    <br>
    <div style="display: inline-block; width: 768px">
        Copyright &copy; Thomio Watanabe
    </div>
</footer>

</body>

</html>
